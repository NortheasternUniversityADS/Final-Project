{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms before feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import the required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime as dt\n",
    "import os\n",
    "import imblearn\n",
    "from imblearn.pipeline import make_pipeline as make_pipeline_imbfinal \n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB \n",
    "#from sklearn import svm\n",
    "from sklearn.metrics import *\n",
    "import zipfile\n",
    "from zipfile import ZipFile\n",
    "from io import BytesIO\n",
    "import sys\n",
    "import glob\n",
    "import datetime\n",
    "import time\n",
    "import boto.s3\n",
    "from boto.s3.key import Key \n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>bp</th>\n",
       "      <th>sg</th>\n",
       "      <th>al</th>\n",
       "      <th>su</th>\n",
       "      <th>bgr</th>\n",
       "      <th>bu</th>\n",
       "      <th>sc</th>\n",
       "      <th>sod</th>\n",
       "      <th>pot</th>\n",
       "      <th>...</th>\n",
       "      <th>PC_normal</th>\n",
       "      <th>PCC_present</th>\n",
       "      <th>BA_present</th>\n",
       "      <th>HTN_yes</th>\n",
       "      <th>DM_yes</th>\n",
       "      <th>CAD_yes</th>\n",
       "      <th>Appet_good</th>\n",
       "      <th>PE_yes</th>\n",
       "      <th>Ane_yes</th>\n",
       "      <th>Classification_ckd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1.020</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>138.0</td>\n",
       "      <td>4.627244</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1.020</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>138.0</td>\n",
       "      <td>4.627244</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1.010</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>423.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>138.0</td>\n",
       "      <td>4.627244</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.005</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>3.8</td>\n",
       "      <td>111.0</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1.010</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>138.0</td>\n",
       "      <td>4.627244</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    age    bp     sg   al   su    bgr    bu   sc    sod       pot  \\\n",
       "0  48.0  80.0  1.020  1.0  0.0  121.0  36.0  1.2  138.0  4.627244   \n",
       "1   7.0  50.0  1.020  4.0  0.0   99.0  18.0  0.8  138.0  4.627244   \n",
       "2  62.0  80.0  1.010  2.0  3.0  423.0  53.0  1.8  138.0  4.627244   \n",
       "3  48.0  70.0  1.005  4.0  0.0  117.0  56.0  3.8  111.0  2.500000   \n",
       "4  51.0  80.0  1.010  2.0  0.0  106.0  26.0  1.4  138.0  4.627244   \n",
       "\n",
       "          ...          PC_normal  PCC_present  BA_present  HTN_yes  DM_yes  \\\n",
       "0         ...                  1            0           0        1       1   \n",
       "1         ...                  1            0           0        0       0   \n",
       "2         ...                  1            0           0        0       1   \n",
       "3         ...                  0            1           0        1       0   \n",
       "4         ...                  1            0           0        0       0   \n",
       "\n",
       "   CAD_yes  Appet_good  PE_yes  Ane_yes  Classification_ckd  \n",
       "0        0           1       0        0                   1  \n",
       "1        0           1       0        0                   1  \n",
       "2        0           0       0        1                   1  \n",
       "3        0           0       1        1                   1  \n",
       "4        0           1       0        0                   1  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('ckd.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157    1\n",
       "109    1\n",
       "17     1\n",
       "347    0\n",
       "24     1\n",
       "Name: Classification_ckd, dtype: int64"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train,df_test = train_test_split(df,train_size=0.7,random_state=42)\n",
    "x_train=df_train.iloc[:,:25]\n",
    "y_train=df_train['Classification_ckd']\n",
    "scaler.fit(x_train)\n",
    "x_train_sc=scaler.transform(x_train)\n",
    "x_test=df_test.iloc[:,:25]\n",
    "y_test=df_test['Classification_ckd']\n",
    "scaler.fit(x_test)\n",
    "x_test_sc=scaler.transform(x_test)\n",
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy =[]\n",
    "model_name =[]\n",
    "dataset=[]\n",
    "f1score = []\n",
    "precision = []\n",
    "recall = []\n",
    "true_positive =[]\n",
    "false_positive =[]\n",
    "true_negative =[]\n",
    "false_negative =[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = LogisticRegression()\n",
    "## fitiing the model\n",
    "logreg.fit(x_train_sc, y_train)\n",
    "filename = 'logReg_model.sav'\n",
    "pickle.dump(logreg,open(filename,'wb'))\n",
    "logreg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[106,   0],\n",
       "       [  0, 174]], dtype=int64)"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = logreg.predict(x_train_sc)\n",
    "f1 = f1_score(y_train, prediction)\n",
    "p = precision_score(y_train, prediction)\n",
    "r = recall_score(y_train, prediction)\n",
    "a = accuracy_score(y_train, prediction)\n",
    "cm = confusion_matrix(y_train, prediction)\n",
    "tp = cm[0][0]\n",
    "fp = cm[0][1]\n",
    "fn = cm[1][0]\n",
    "tn = cm[1][1]\n",
    "dataset.append('Training')\n",
    "model_name.append('Logistic Regression')\n",
    "f1score.append(f1)\n",
    "precision.append(p)\n",
    "recall.append(r)\n",
    "accuracy.append(a)\n",
    "true_positive.append(tp) \n",
    "false_positive.append(fp)\n",
    "true_negative.append(tn) \n",
    "false_negative.append(fn)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[44,  0],\n",
       "       [ 0, 76]], dtype=int64)"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = logreg.predict(x_test_sc)\n",
    "f1 = f1_score(y_test,  prediction)\n",
    "p = precision_score(y_test,  prediction)\n",
    "r = recall_score(y_test,  prediction)\n",
    "a = accuracy_score(y_test,  prediction)\n",
    "cm = confusion_matrix(y_test,  prediction)\n",
    "tp = cm[0][0]\n",
    "fp = cm[0][1]\n",
    "fn = cm[1][0]\n",
    "tn = cm[1][1]\n",
    "model_name.append('Logistc Regression')\n",
    "dataset.append('Testing')\n",
    "f1score.append(f1)\n",
    "precision.append(p)\n",
    "recall.append(r)\n",
    "accuracy.append(a)\n",
    "true_positive.append(tp) \n",
    "false_positive.append(fp)\n",
    "true_negative.append(tn) \n",
    "false_negative.append(fn)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Bernoulli Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NB = BernoulliNB()\n",
    "## fitiing the model\n",
    "NB.fit(x_train_sc, y_train)\n",
    "filename = 'BernoulliNB_model.sav'\n",
    "pickle.dump(NB,open(filename,'wb'))\n",
    "NB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[106,   0],\n",
       "       [  2, 172]], dtype=int64)"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = NB.predict(x_train_sc)\n",
    "f1 = f1_score(y_train, prediction)\n",
    "p = precision_score(y_train, prediction)\n",
    "r = recall_score(y_train, prediction)\n",
    "a = accuracy_score(y_train, prediction)\n",
    "cm = confusion_matrix(y_train, prediction)\n",
    "tp = cm[0][0]\n",
    "fp = cm[0][1]\n",
    "fn = cm[1][0]\n",
    "tn = cm[1][1]\n",
    "model_name.append('Bernoulli Naive Bayes')\n",
    "dataset.append('Training')\n",
    "f1score.append(f1)\n",
    "precision.append(p)\n",
    "recall.append(r)\n",
    "accuracy.append(a)\n",
    "true_positive.append(tp) \n",
    "false_positive.append(fp)\n",
    "true_negative.append(tn) \n",
    "false_negative.append(fn)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[44,  0],\n",
       "       [ 1, 75]], dtype=int64)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = NB.predict(x_test_sc)\n",
    "f1 = f1_score(y_test,  prediction)\n",
    "p = precision_score(y_test,  prediction)\n",
    "r = recall_score(y_test,  prediction)\n",
    "a = accuracy_score(y_test,  prediction)\n",
    "cm = confusion_matrix(y_test,  prediction)\n",
    "tp = cm[0][0]\n",
    "fp = cm[0][1]\n",
    "fn = cm[1][0]\n",
    "tn = cm[1][1]\n",
    "model_name.append('Bernoulli Naive Bayes')\n",
    "dataset.append('Testing')\n",
    "f1score.append(f1)\n",
    "precision.append(p)\n",
    "recall.append(r)\n",
    "accuracy.append(a)\n",
    "true_positive.append(tp) \n",
    "false_positive.append(fp)\n",
    "true_negative.append(tn) \n",
    "false_negative.append(fn)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=1,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=50,random_state=0)\n",
    "## fitiing the model\n",
    "rfc.fit(x_train_sc, y_train)\n",
    "filename = 'RFC_model.sav'\n",
    "pickle.dump(rfc,open(filename,'wb'))\n",
    "rfc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[106,   0],\n",
       "       [  0, 174]], dtype=int64)"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = rfc.predict(x_train_sc)\n",
    "f1 = f1_score(y_train, prediction)\n",
    "p = precision_score(y_train, prediction)\n",
    "r = recall_score(y_train, prediction)\n",
    "a = accuracy_score(y_train, prediction)\n",
    "cm = confusion_matrix(y_train, prediction)\n",
    "tp = cm[0][0]\n",
    "fp = cm[0][1]\n",
    "fn = cm[1][0]\n",
    "tn = cm[1][1]\n",
    "model_name.append('Random Forest Classifier')\n",
    "dataset.append('Training')\n",
    "f1score.append(f1)\n",
    "precision.append(p)\n",
    "recall.append(r)\n",
    "accuracy.append(a)\n",
    "true_positive.append(tp) \n",
    "false_positive.append(fp)\n",
    "true_negative.append(tn) \n",
    "false_negative.append(fn)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[44,  0],\n",
       "       [ 0, 76]], dtype=int64)"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = rfc.predict(x_test_sc)\n",
    "f1 = f1_score(y_test,  prediction)\n",
    "p = precision_score(y_test,  prediction)\n",
    "r = recall_score(y_test,  prediction)\n",
    "a = accuracy_score(y_test,  prediction)\n",
    "cm = confusion_matrix(y_test,  prediction)\n",
    "tp = cm[0][0]\n",
    "fp = cm[0][1]\n",
    "fn = cm[1][0]\n",
    "tn = cm[1][1]\n",
    "model_name.append('Random Forest Classifier')\n",
    "dataset.append('Testing')\n",
    "f1score.append(f1)\n",
    "precision.append(p)\n",
    "recall.append(r)\n",
    "accuracy.append(a)\n",
    "true_positive.append(tp) \n",
    "false_positive.append(fp)\n",
    "true_negative.append(tn) \n",
    "false_negative.append(fn)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k Nearest Neighbour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=2, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = 2)\n",
    "knn.fit(x_train_sc, y_train)\n",
    "filename = 'KNN_model.sav'\n",
    "pickle.dump(knn,open(filename,'wb'))\n",
    "knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[106,   0],\n",
       "       [  0, 174]], dtype=int64)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = knn.predict(x_train_sc)\n",
    "f1 = f1_score(y_train, prediction)\n",
    "p = precision_score(y_train, prediction)\n",
    "r = recall_score(y_train, prediction)\n",
    "a = accuracy_score(y_train, prediction)\n",
    "cm = confusion_matrix(y_train, prediction)\n",
    "tp = cm[0][0]\n",
    "fp = cm[0][1]\n",
    "fn = cm[1][0]\n",
    "tn = cm[1][1]\n",
    "model_name.append('k nearest Neighbour')\n",
    "dataset.append('Training')\n",
    "f1score.append(f1)\n",
    "precision.append(p)\n",
    "recall.append(r)\n",
    "accuracy.append(a)\n",
    "true_positive.append(tp) \n",
    "false_positive.append(fp)\n",
    "true_negative.append(tn) \n",
    "false_negative.append(fn)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[44,  0],\n",
       "       [ 0, 76]], dtype=int64)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = knn.predict(x_test_sc)\n",
    "f1 = f1_score(y_test,  prediction)\n",
    "p = precision_score(y_test,  prediction)\n",
    "r = recall_score(y_test,  prediction)\n",
    "a = accuracy_score(y_test,  prediction)\n",
    "cm = confusion_matrix(y_test,  prediction)\n",
    "tp = cm[0][0]\n",
    "fp = cm[0][1]\n",
    "fn = cm[1][0]\n",
    "tn = cm[1][1]\n",
    "model_name.append('k nearest Neighbour')\n",
    "dataset.append('Testing')\n",
    "f1score.append(f1)\n",
    "precision.append(p)\n",
    "recall.append(r)\n",
    "accuracy.append(a)\n",
    "true_positive.append(tp) \n",
    "false_positive.append(fp)\n",
    "true_negative.append(tn) \n",
    "false_negative.append(fn)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=0.025, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=42, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc = SVC (kernel = 'linear' , C = 0.025 , random_state = 42)\n",
    "svc.fit(x_train_sc, y_train)\n",
    "filename = 'SVC_model.sav'\n",
    "pickle.dump(svc,open(filename,'wb'))\n",
    "svc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[106,   0],\n",
       "       [  0, 174]], dtype=int64)"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = svc.predict(x_train_sc)\n",
    "f1 = f1_score(y_train, prediction)\n",
    "p = precision_score(y_train, prediction)\n",
    "r = recall_score(y_train, prediction)\n",
    "a = accuracy_score(y_train, prediction)\n",
    "cm = confusion_matrix(y_train, prediction)\n",
    "tp = cm[0][0]\n",
    "fp = cm[0][1]\n",
    "fn = cm[1][0]\n",
    "tn = cm[1][1]\n",
    "model_name.append('Support Vector Classifier')\n",
    "dataset.append('Training')\n",
    "f1score.append(f1)\n",
    "precision.append(p)\n",
    "recall.append(r)\n",
    "accuracy.append(a)\n",
    "true_positive.append(tp) \n",
    "false_positive.append(fp)\n",
    "true_negative.append(tn) \n",
    "false_negative.append(fn)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[44,  0],\n",
       "       [ 0, 76]], dtype=int64)"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = svc.predict(x_test_sc)\n",
    "f1 = f1_score(y_test,  prediction)\n",
    "p = precision_score(y_test,  prediction)\n",
    "r = recall_score(y_test,  prediction)\n",
    "a = accuracy_score(y_test,  prediction)\n",
    "cm = confusion_matrix(y_test,  prediction)\n",
    "tp = cm[0][0]\n",
    "fp = cm[0][1]\n",
    "fn = cm[1][0]\n",
    "tn = cm[1][1]\n",
    "model_name.append('Support Vector Classifier')\n",
    "dataset.append('Testing')\n",
    "f1score.append(f1)\n",
    "precision.append(p)\n",
    "recall.append(r)\n",
    "accuracy.append(a)\n",
    "true_positive.append(tp) \n",
    "false_positive.append(fp)\n",
    "true_negative.append(tn) \n",
    "false_negative.append(fn)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Decent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='modified_huber', max_iter=None,\n",
       "       n_iter=None, n_jobs=1, penalty='l2', power_t=0.5, random_state=42,\n",
       "       shuffle=True, tol=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "sgd = SGDClassifier (loss = 'modified_huber' , shuffle = True , random_state = 42)\n",
    "sgd.fit(x_train_sc, y_train)\n",
    "filename = 'SGD_model.sav'\n",
    "pickle.dump(sgd,open(filename,'wb'))\n",
    "sgd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[106,   0],\n",
       "       [  0, 174]], dtype=int64)"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = sgd.predict(x_train_sc)\n",
    "f1 = f1_score(y_train, prediction)\n",
    "p = precision_score(y_train, prediction)\n",
    "r = recall_score(y_train, prediction)\n",
    "a = accuracy_score(y_train, prediction)\n",
    "cm = confusion_matrix(y_train, prediction)\n",
    "tp = cm[0][0]\n",
    "fp = cm[0][1]\n",
    "fn = cm[1][0]\n",
    "tn = cm[1][1]\n",
    "model_name.append('Stochastic Gradient Decent')\n",
    "dataset.append('Training')\n",
    "f1score.append(f1)\n",
    "precision.append(p)\n",
    "recall.append(r)\n",
    "accuracy.append(a)\n",
    "true_positive.append(tp) \n",
    "false_positive.append(fp)\n",
    "true_negative.append(tn) \n",
    "false_negative.append(fn)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[44,  0],\n",
       "       [ 0, 76]], dtype=int64)"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = sgd.predict(x_test_sc)\n",
    "f1 = f1_score(y_test,  prediction)\n",
    "p = precision_score(y_test,  prediction)\n",
    "r = recall_score(y_test,  prediction)\n",
    "a = accuracy_score(y_test,  prediction)\n",
    "cm = confusion_matrix(y_test,  prediction)\n",
    "tp = cm[0][0]\n",
    "fp = cm[0][1]\n",
    "fn = cm[1][0]\n",
    "tn = cm[1][1]\n",
    "model_name.append('Stochastic Gradient Decent')\n",
    "dataset.append('Testing')\n",
    "f1score.append(f1)\n",
    "precision.append(p)\n",
    "recall.append(r)\n",
    "accuracy.append(a)\n",
    "true_positive.append(tp) \n",
    "false_positive.append(fp)\n",
    "true_negative.append(tn) \n",
    "false_negative.append(fn)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None)"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(x_train_sc, y_train)\n",
    "filename = 'GNB_model.sav'\n",
    "pickle.dump(gnb,open(filename,'wb'))\n",
    "gnb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[106,   0],\n",
       "       [  0, 174]], dtype=int64)"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = gnb.predict(x_train_sc)\n",
    "f1 = f1_score(y_train, prediction)\n",
    "p = precision_score(y_train, prediction)\n",
    "r = recall_score(y_train, prediction)\n",
    "a = accuracy_score(y_train, prediction)\n",
    "cm = confusion_matrix(y_train, prediction)\n",
    "tp = cm[0][0]\n",
    "fp = cm[0][1]\n",
    "fn = cm[1][0]\n",
    "tn = cm[1][1]\n",
    "model_name.append('Gradient Naive Bayes')\n",
    "dataset.append('Training')\n",
    "f1score.append(f1)\n",
    "precision.append(p)\n",
    "recall.append(r)\n",
    "accuracy.append(a)\n",
    "true_positive.append(tp) \n",
    "false_positive.append(fp)\n",
    "true_negative.append(tn) \n",
    "false_negative.append(fn)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[44,  0],\n",
       "       [ 0, 76]], dtype=int64)"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = gnb.predict(x_test_sc)\n",
    "f1 = f1_score(y_test,  prediction)\n",
    "p = precision_score(y_test,  prediction)\n",
    "r = recall_score(y_test,  prediction)\n",
    "a = accuracy_score(y_test,  prediction)\n",
    "cm = confusion_matrix(y_test,  prediction)\n",
    "tp = cm[0][0]\n",
    "fp = cm[0][1]\n",
    "fn = cm[1][0]\n",
    "tn = cm[1][1]\n",
    "model_name.append('Gradient Naive Bayes')\n",
    "dataset.append('Testing')\n",
    "f1score.append(f1)\n",
    "precision.append(p)\n",
    "recall.append(r)\n",
    "accuracy.append(a)\n",
    "true_positive.append(tp) \n",
    "false_positive.append(fp)\n",
    "true_negative.append(tn) \n",
    "false_negative.append(fn)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing summary metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "Summary = model_name,dataset,f1score,precision,recall,accuracy,true_positive,false_positive,true_negative,false_negative\n",
    "#Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model_Name</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>F1_score</th>\n",
       "      <th>Precision_score</th>\n",
       "      <th>Recall_score</th>\n",
       "      <th>Accuracy_score</th>\n",
       "      <th>True_Positive</th>\n",
       "      <th>False_Positive</th>\n",
       "      <th>True_Negative</th>\n",
       "      <th>False_Negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>Training</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>106</td>\n",
       "      <td>0</td>\n",
       "      <td>174</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistc Regression</td>\n",
       "      <td>Testing</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>Training</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>106</td>\n",
       "      <td>0</td>\n",
       "      <td>174</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>Testing</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>k nearest Neighbour</td>\n",
       "      <td>Training</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>106</td>\n",
       "      <td>0</td>\n",
       "      <td>174</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>k nearest Neighbour</td>\n",
       "      <td>Testing</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Support Vector Classifier</td>\n",
       "      <td>Training</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>106</td>\n",
       "      <td>0</td>\n",
       "      <td>174</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Support Vector Classifier</td>\n",
       "      <td>Testing</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Stochastic Gradient Decent</td>\n",
       "      <td>Training</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>106</td>\n",
       "      <td>0</td>\n",
       "      <td>174</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Stochastic Gradient Decent</td>\n",
       "      <td>Testing</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Gradient Naive Bayes</td>\n",
       "      <td>Training</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>106</td>\n",
       "      <td>0</td>\n",
       "      <td>174</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Gradient Naive Bayes</td>\n",
       "      <td>Testing</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Bernoulli Naive Bayes</td>\n",
       "      <td>Testing</td>\n",
       "      <td>0.993377</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.986842</td>\n",
       "      <td>0.991667</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Bernoulli Naive Bayes</td>\n",
       "      <td>Training</td>\n",
       "      <td>0.994220</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.988506</td>\n",
       "      <td>0.992857</td>\n",
       "      <td>106</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Model_Name                Dataset  F1_score  Precision_score  \\\n",
       "0          Logistic Regression  Training  1.000000              1.0   \n",
       "1           Logistc Regression   Testing  1.000000              1.0   \n",
       "2     Random Forest Classifier  Training  1.000000              1.0   \n",
       "3     Random Forest Classifier   Testing  1.000000              1.0   \n",
       "4          k nearest Neighbour  Training  1.000000              1.0   \n",
       "5          k nearest Neighbour   Testing  1.000000              1.0   \n",
       "6    Support Vector Classifier  Training  1.000000              1.0   \n",
       "7    Support Vector Classifier   Testing  1.000000              1.0   \n",
       "8   Stochastic Gradient Decent  Training  1.000000              1.0   \n",
       "9   Stochastic Gradient Decent   Testing  1.000000              1.0   \n",
       "10        Gradient Naive Bayes  Training  1.000000              1.0   \n",
       "11        Gradient Naive Bayes   Testing  1.000000              1.0   \n",
       "12       Bernoulli Naive Bayes   Testing  0.993377              1.0   \n",
       "13       Bernoulli Naive Bayes  Training  0.994220              1.0   \n",
       "\n",
       "    Recall_score  Accuracy_score  True_Positive  False_Positive  \\\n",
       "0       1.000000        1.000000            106               0   \n",
       "1       1.000000        1.000000             44               0   \n",
       "2       1.000000        1.000000            106               0   \n",
       "3       1.000000        1.000000             44               0   \n",
       "4       1.000000        1.000000            106               0   \n",
       "5       1.000000        1.000000             44               0   \n",
       "6       1.000000        1.000000            106               0   \n",
       "7       1.000000        1.000000             44               0   \n",
       "8       1.000000        1.000000            106               0   \n",
       "9       1.000000        1.000000             44               0   \n",
       "10      1.000000        1.000000            106               0   \n",
       "11      1.000000        1.000000             44               0   \n",
       "12      0.986842        0.991667             44               0   \n",
       "13      0.988506        0.992857            106               0   \n",
       "\n",
       "    True_Negative  False_Negative  \n",
       "0             174               0  \n",
       "1              76               0  \n",
       "2             174               0  \n",
       "3              76               0  \n",
       "4             174               0  \n",
       "5              76               0  \n",
       "6             174               0  \n",
       "7              76               0  \n",
       "8             174               0  \n",
       "9              76               0  \n",
       "10            174               0  \n",
       "11             76               0  \n",
       "12             75               1  \n",
       "13            172               2  "
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Making a dataframe of the accuracy and error metrics\n",
    "describe1 = pd.DataFrame(Summary[0],columns = {\"Model_Name             \"})\n",
    "describe2 = pd.DataFrame(Summary[1],columns = {\"Dataset\"})\n",
    "describe3 = pd.DataFrame(Summary[2],columns = {\"F1_score\"})\n",
    "describe4 = pd.DataFrame(Summary[3],columns = {\"Precision_score\"})\n",
    "describe5 = pd.DataFrame(Summary[4],columns = {\"Recall_score\"})\n",
    "describe6 = pd.DataFrame(Summary[5], columns ={\"Accuracy_score\"})\n",
    "describe7 = pd.DataFrame(Summary[6], columns ={\"True_Positive\"})\n",
    "describe8 = pd.DataFrame(Summary[7], columns ={\"False_Positive\"})\n",
    "describe9 = pd.DataFrame(Summary[8], columns ={\"True_Negative\"})\n",
    "describe10 = pd.DataFrame(Summary[9], columns ={\"False_Negative\"})\n",
    "des = describe1.merge(describe2, left_index=True, right_index=True, how='inner')\n",
    "des = des.merge(describe3,left_index=True, right_index=True, how='inner')\n",
    "des = des.merge(describe4,left_index=True, right_index=True, how='inner')\n",
    "des = des.merge(describe5,left_index=True, right_index=True, how='inner')\n",
    "des = des.merge(describe6,left_index=True, right_index=True, how='inner')\n",
    "des = des.merge(describe7,left_index=True, right_index=True, how='inner')\n",
    "des = des.merge(describe8,left_index=True, right_index=True, how='inner')\n",
    "des = des.merge(describe9,left_index=True, right_index=True, how='inner')\n",
    "des = des.merge(describe10,left_index=True, right_index=True, how='inner')\n",
    "#des = des.merge(describe9,left_index=True, right_index=True, how='inner')\n",
    "Summary_csv = des.sort_values(ascending=True,by=\"False_Negative\").reset_index(drop = True)\n",
    "Summary_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "Summary_csv.to_csv('Summary.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: Except for `Bernoulli Naive Bayes` , we are getting 100% accuracy for every other model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
